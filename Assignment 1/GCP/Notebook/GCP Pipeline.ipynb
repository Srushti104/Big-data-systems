{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the following packages before running the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "pip install --upgrade tensorflow==1.4\n",
    "pip install --ignore-installed --upgrade pytz==2018.4\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "pip install --upgrade apache-beam[gcp]==2.12.0\n",
    "pip install google-cloud-bigquery --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create DataSet on BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bq mk --dataset sevir-306302:sevirdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Tables on BigQuery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "bq mk -t sevirdataset.catalogcsv \\\n",
    "id:STRING,file_name:STRING,file_index:INTEGER,img_type:STRING,time_utc:DATETIME,episode_id:STRING,event_id:STRING,event_type:STRING,llcrnrlat:FLOAT,llcrnrlon:FLOAT,urcrnrlat:FLOAT,urcrnrlon:FLOAT,proj:STRING,size_x:INTEGER,size_y:INTEGER,height_m:FLOAT,width_m:FLOAT,data_min:FLOAT,data_max:FLOAT,pct_missing:FLOAT\n",
    "bq mk -t sevirdataset.storm_details \\\n",
    "BEGIN_YEARMONTH:INTEGER,BEGIN_DAY:INTEGER,BEGIN_TIME:INTEGER,END_YEARMONTH:INTEGER,END_DAY:INTEGER,END_TIME:INTEGER,EPISODE_ID:STRING,EVENT_ID:INTEGER,STATE:STRING,STATE_FIPS:INTEGER,YEAR:INTEGER,MONTH_NAME:STRING,EVENT_TYPE:STRING,CZ_TYPE:STRING,CZ_FIPS:INTEGER,CZ_NAME:STRING,WFO:STRING,BEGIN_DATE_TIME:STRING,CZ_TIMEZONE:STRING,END_DATE_TIME:STRING,INJURIES_DIRECT:STRING,INJURIES_INDIRECT:STRING,DEATHS_DIRECT:STRING,DEATHS_INDIRECT:STRING,DAMAGE_PROPERTY:STRING,DAMAGE_CROPS:STRING,SOURCE:STRING,MAGNITUDE:STRING,MAGNITUDE_TYPE:STRING,FLOOD_CAUSE:STRING,CATEGORY:STRING,BEGIN_RANGE:STRING,BEGIN_AZIMUTH:STRING,BEGIN_LOCATION:STRING,END_RANGE:STRING,END_AZIMUTH:STRING,END_LOCATION:STRING,BEGIN_LAT:STRING,BEGIN_LON:STRING,END_LAT:STRING,END_LON:STRING,EPISODE_NARRATIVE:STRING,EVENT_NARRATIVE:STRING,DATA_SOURCE:STRING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pipeline to move data from cloud Storage to Bigquery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import apache_beam as beam\n",
    "import argparse\n",
    "from apache_beam.options.pipeline_options import PipelineOptions\n",
    "from sys import argv\n",
    "PROJECT_ID = 'sevir-306302'\n",
    "SCHEMACATALOG = 'id:STRING,file_name:STRING,file_index:INTEGER,img_type:STRING,time_utc:DATETIME,episode_id:STRING,event_id:STRING,event_type:STRING,llcrnrlat:FLOAT,llcrnrlon:FLOAT,urcrnrlat:FLOAT,urcrnrlon:FLOAT,proj:STRING,size_x:INTEGER,size_y:INTEGER,height_m:FLOAT,width_m:FLOAT,data_min:FLOAT,data_max:FLOAT,pct_missing:FLOAT'\n",
    "SCHEMASTORM = 'BEGIN_YEARMONTH:INTEGER,BEGIN_DAY:INTEGER,BEGIN_TIME:INTEGER,END_YEARMONTH:INTEGER,END_DAY:INTEGER,END_TIME:INTEGER,EPISODE_ID:STRING,EVENT_ID:INTEGER,STATE:STRING,STATE_FIPS:INTEGER,YEAR:INTEGER,MONTH_NAME:STRING,EVENT_TYPE:STRING,CZ_TYPE:STRING,CZ_FIPS:INTEGER,CZ_NAME:STRING,WFO:STRING,BEGIN_DATE_TIME:STRING,CZ_TIMEZONE:STRING,END_DATE_TIME:STRING,INJURIES_DIRECT:STRING,INJURIES_INDIRECT:STRING,DEATHS_DIRECT:STRING,DEATHS_INDIRECT:STRING,DAMAGE_PROPERTY:STRING,DAMAGE_CROPS:STRING,SOURCE:STRING,MAGNITUDE:STRING,MAGNITUDE_TYPE:STRING,FLOOD_CAUSE:STRING,CATEGORY:STRING,BEGIN_RANGE:STRING,BEGIN_AZIMUTH:STRING,BEGIN_LOCATION:STRING,END_RANGE:STRING,END_AZIMUTH:STRING,END_LOCATION:STRING,BEGIN_LAT:STRING,BEGIN_LON:STRING,END_LAT:STRING,END_LON:STRING,EPISODE_NARRATIVE:STRING,EVENT_NARRATIVE:STRING,DATA_SOURCE:STRING'\n",
    "\n",
    "\n",
    "def dropcolums(data):\n",
    "    del data['minute_offsets']\n",
    "    return data\n",
    "\n",
    "def convert_types(data):\n",
    "    \"\"\"Converts string values to their appropriate type.\"\"\"\n",
    "    data['height_m'] = float(data['height_m']) if 'height_m' in data else None\n",
    "    data['width_m'] = float(data['width_m']) if 'width_m' in data else None\n",
    "    return data\n",
    "\n",
    "def dropcolumsstorm(data):\n",
    "    del data['TOR_F_SCALE'],\n",
    "    del data['TOR_LENGTH'],\n",
    "    del data['TOR_WIDTH'],\n",
    "    del data['TOR_OTHER_WFO'],\n",
    "    del data['TOR_OTHER_CZ_STATE'],\n",
    "    del data['TOR_OTHER_CZ_FIPS'],\n",
    "    del data['TOR_OTHER_CZ_NAME']\n",
    "    return data\n",
    "\n",
    "  \n",
    "p = beam.Pipeline(options=PipelineOptions(flags=argv,\n",
    "    runner='DataflowRunner',\n",
    "    project='sevir-306302',\n",
    "    job_name='catalog',\n",
    "    temp_location='gs://sevir-306302/temp',\n",
    "    staging_location='gs://sevir-306302/tempstg',\n",
    "    region='us-central1'))\n",
    "(p | 'ReadDataCatalog' >> beam.io.ReadFromText('gs://sevir-306302/Catalog/CATALOG.csv', skip_header_lines =1)\n",
    "       | 'SplitCatalog' >> beam.Map(lambda x: x.split(','))\n",
    "       | 'format to dictCatalog' >> beam.Map(lambda x: {\"id\": x[0], \"file_name\": x[1], \"file_index\": x[2], \n",
    "                                                 \"img_type\": x[3],\"time_utc\": x[4], \"minute_offsets\": x[5], \"episode_id\": x[6], \n",
    "                                                 \"event_id\": x[7], \"event_type\": x[8], \"llcrnrlat\": x[9], \"llcrnrlon\": x[10], \n",
    "                                                 \"urcrnrlat\": x[11],\"urcrnrlon\": x[12], \"proj\": x[13], \"size_x\": x[14], \n",
    "                                                 \"size_y\": x[15], \"height_m\": x[16], \"width_m\": x[17],\"data_min\": x[18], \n",
    "                                                 \"data_max\": x[19], \"pct_missing\": x[20]})\n",
    "                                     \n",
    "      # | 'DelIncompleteData' >> beam.Filter(discard_incomplete)\n",
    "       | 'ConvertypesCatalog' >> beam.Map(convert_types)\n",
    "       | 'DelUnwantedDataCatalog' >> beam.Map(dropcolums)\n",
    "\n",
    "       | 'WriteToBigQuery' >> beam.io.WriteToBigQuery(\n",
    "           '{0}:sevirdataset.catalogcsv'.format(PROJECT_ID),\n",
    "           schema=SCHEMACATALOG,\n",
    "           write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND))\n",
    "\n",
    "\n",
    "(p | 'ReadDataStorm' >> beam.io.ReadFromText('gs://sevir-306302/Storm/StormEvents_details_new.csv', skip_header_lines =1)\n",
    "       | 'SplitStormData' >> beam.Map(lambda x: x.split(','))\n",
    "       | 'format to dict Storm' >> beam.Map(lambda x: {\"BEGIN_YEARMONTH\": x[0], \"BEGIN_DAY\": x[1], \"BEGIN_TIME\": x[2], \n",
    "                                                 \"END_YEARMONTH\": x[3],\"END_DAY\": x[4], \"END_TIME\": x[5], \"EPISODE_ID\": x[6], \n",
    "                                                 \"EVENT_ID\": x[7], \"STATE\": x[8], \"STATE_FIPS\": x[9], \"YEAR\": x[10], \n",
    "                                                 \"MONTH_NAME\": x[11],\"EVENT_TYPE\": x[12], \"CZ_TYPE\": x[13], \"CZ_FIPS\": x[14], \n",
    "                                                 \"CZ_NAME\": x[15], \"WFO\": x[16], \"BEGIN_DATE_TIME\": x[17],\"CZ_TIMEZONE\": x[18], \n",
    "                                                 \"END_DATE_TIME\": x[19], \"INJURIES_DIRECT\": x[20],\n",
    "                                                 \"INJURIES_INDIRECT\": x[21], \"DEATHS_DIRECT\": x[22], \"DEATHS_INDIRECT\": x[23],\"DAMAGE_PROPERTY\": x[24],\n",
    "                                                 \"DAMAGE_CROPS\": x[25], \"SOURCE\": x[26], \"MAGNITUDE\": x[27],\"MAGNITUDE_TYPE\": x[28],\n",
    "                                                 \"FLOOD_CAUSE\": x[29], \"CATEGORY\": x[30], \"TOR_F_SCALE\": x[31],\"TOR_LENGTH\": x[32],\n",
    "                                                 \"TOR_WIDTH\": x[33], \"TOR_OTHER_WFO\": x[34], \"TOR_OTHER_CZ_STATE\": x[35],\"TOR_OTHER_CZ_FIPS\": x[36],\n",
    "                                                 \"TOR_OTHER_CZ_NAME\": x[37], \"BEGIN_RANGE\": x[38], \"BEGIN_AZIMUTH\": x[39],\"BEGIN_LOCATION\": x[40],\n",
    "                                                 \"END_RANGE\": x[41], \"END_AZIMUTH\": x[42], \"END_LOCATION\": x[43],\"BEGIN_LAT\": x[44],\n",
    "                                                 \"BEGIN_LON\": x[45], \"END_LAT\": x[46], \"END_LON\": x[47],\"EPISODE_NARRATIVE\": x[48],\n",
    "                                                 \"EVENT_NARRATIVE\": x[49], \"DATA_SOURCE\": x[50]})\n",
    "                                     \n",
    "      # | 'DelIncompleteData' >> beam.Filter(discard_incomplete)\n",
    "      # | 'Convertypes_storm' >> beam.Map(convert_types_storm)\n",
    "       | 'DelUnwantedDataStorm' >> beam.Map(dropcolumsstorm)\n",
    "\n",
    "       | 'WriteToBigQueryStorm' >> beam.io.WriteToBigQuery(\n",
    "           '{0}:sevirdataset.storm_details'.format(PROJECT_ID),\n",
    "           schema=SCHEMASTORM,\n",
    "           write_disposition=beam.io.BigQueryDisposition.WRITE_APPEND))\n",
    "result = p.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
